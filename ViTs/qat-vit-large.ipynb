{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"93cf2b55","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport timm\nimport torch.ao.quantization.quantize_fx as quantize_fx\nimport time\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom typing import Tuple, Dict, List\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:09:03.069299Z","iopub.execute_input":"2025-08-28T09:09:03.069589Z","iopub.status.idle":"2025-08-28T09:09:15.101215Z","shell.execute_reply.started":"2025-08-28T09:09:03.069569Z","shell.execute_reply":"2025-08-28T09:09:15.100631Z"}},"outputs":[],"execution_count":1},{"id":"e0377c17","cell_type":"code","source":"class PerformanceMetrics:\n    \"\"\"Class to track and compare model performance metrics\"\"\"\n    \n    def __init__(self):\n        self.metrics = {}\n    \n    def add_model_metrics(self, model_name: str, accuracy: float, \n                         inference_time: float, model_size: float,\n                         memory_usage: float = None):\n        \"\"\"Add metrics for a specific model\"\"\"\n        self.metrics[model_name] = {\n            'accuracy': accuracy,\n            'inference_time': inference_time,\n            'model_size': model_size,\n            'memory_usage': memory_usage\n        }\n    \n    def compare_models(self):\n        \"\"\"Generate comparison report\"\"\"\n        if len(self.metrics) < 2:\n            print(\"Need at least 2 models for comparison\")\n            return\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"MODEL PERFORMANCE COMPARISON\")\n        print(\"=\"*80)\n        \n        # Print tabular comparison\n        print(f\"{'Model':<15} {'Accuracy':<12} {'Inf. Time (ms)':<15} {'Size (MB)':<12} {'Memory (MB)':<12}\")\n        print(\"-\" * 80)\n        \n        for name, metrics in self.metrics.items():\n            memory_str = f\"{metrics['memory_usage']:.2f}\" if metrics['memory_usage'] else \"N/A\"\n            print(f\"{name:<15} {metrics['accuracy']:.4f}       \"\n                  f\"{metrics['inference_time']*1000:.2f}           \"\n                  f\"{metrics['model_size']:.2f}        {memory_str}\")\n        \n        # Calculate improvements/degradations\n        if 'FP32' in self.metrics and 'QAT_INT8' in self.metrics:\n            fp32 = self.metrics['FP32']\n            qat = self.metrics['QAT_INT8']\n            \n            acc_change = ((qat['accuracy'] - fp32['accuracy']) / fp32['accuracy']) * 100\n            speed_improvement = ((fp32['inference_time'] - qat['inference_time']) / fp32['inference_time']) * 100\n            size_reduction = ((fp32['model_size'] - qat['model_size']) / fp32['model_size']) * 100\n            \n            print(\"\\n\" + \"=\"*50)\n            print(\"QAT vs FP32 IMPROVEMENTS:\")\n            print(\"=\"*50)\n            print(f\"Accuracy change: {acc_change:+.2f}%\")\n            print(f\"Speed improvement: {speed_improvement:+.2f}%\")\n            print(f\"Model size reduction: {size_reduction:+.2f}%\")\n            \n            if qat['memory_usage'] and fp32['memory_usage']:\n                mem_reduction = ((fp32['memory_usage'] - qat['memory_usage']) / fp32['memory_usage']) * 100\n                print(f\"Memory usage reduction: {mem_reduction:+.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:09:15.101980Z","iopub.execute_input":"2025-08-28T09:09:15.102380Z","iopub.status.idle":"2025-08-28T09:09:15.110415Z","shell.execute_reply.started":"2025-08-28T09:09:15.102355Z","shell.execute_reply":"2025-08-28T09:09:15.109861Z"}},"outputs":[],"execution_count":2},{"id":"d5ab9774","cell_type":"code","source":"def get_model_size(model):\n    \"\"\"Calculate model size in MB\"\"\"\n    param_size = 0\n    buffer_size = 0\n    \n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    \n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    \n    size_all_mb = (param_size + buffer_size) / 1024**2\n    return size_all_mb\n\ndef get_memory_usage():\n    \"\"\"Get current GPU memory usage in MB\"\"\"\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / 1024**2\n    return None\n\ndef evaluate_model(model, data_loader, device, num_classes=10):\n    \"\"\"Evaluate model and return accuracy and detailed metrics\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    inference_times = []\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            \n            # Measure inference time\n            start_time = time.time()\n            outputs = model(data)\n            end_time = time.time()\n            \n            inference_times.append(end_time - start_time)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n    \n    accuracy = correct / total\n    avg_inference_time = np.mean(inference_times)\n    \n    return accuracy, avg_inference_time, all_preds, all_targets\n\ndef prepare_data(batch_size=64, num_workers=2):\n    \"\"\"Prepare CIFAR-100 dataset with appropriate transforms for ViT\"\"\"\n    \n    # Transforms for training and testing\n    transform_train = transforms.Compose([\n        transforms.Resize((224, 224)),  # ViT expects 224x224 images\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomGrayscale(p=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet normalization\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    \n    # Load CIFAR-100 dataset\n    train_dataset = torchvision.datasets.CIFAR100(\n        root='./data', train=True, download=True, transform=transform_train\n    )\n    \n    test_dataset = torchvision.datasets.CIFAR100(\n        root='./data', train=False, download=True, transform=transform_test\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n    )\n    \n    return train_loader, test_loader\n\ndef create_vit_model(num_classes=100):\n    \"\"\"Create and modify ViT Large model for CIFAR-100\"\"\"\n    # Load pre-trained ViT Large model\n    model = timm.create_model('vit_large_patch16_224', pretrained=True)\n    \n    # Modify the classifier for CIFAR-100 (100 classes)\n    model.head = nn.Linear(model.head.in_features, num_classes)\n    \n    return model\n\ndef train_model(model, train_loader, criterion, optimizer, device, num_epochs=3):\n    \"\"\"Training function\"\"\"\n    model.to(device)\n    model.train()\n    \n    print(f\"\\nTraining on {device} for {num_epochs} epochs...\")\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for i, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            if i % 100 == 99:  # Print every 100 batches\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], '\n                      f'Loss: {running_loss/100:.4f}, '\n                      f'Acc: {100*correct/total:.2f}%')\n                running_loss = 0.0\n        \n        epoch_acc = 100 * correct / total\n        print(f'Epoch [{epoch+1}/{num_epochs}] completed. Training Accuracy: {epoch_acc:.2f}%')\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n    \"\"\"Plot confusion matrix\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:09:15.112094Z","iopub.execute_input":"2025-08-28T09:09:15.112464Z","iopub.status.idle":"2025-08-28T09:09:15.132516Z","shell.execute_reply.started":"2025-08-28T09:09:15.112447Z","shell.execute_reply":"2025-08-28T09:09:15.131830Z"}},"outputs":[],"execution_count":3},{"id":"8ef430b7","cell_type":"code","source":"def main():\n    \"\"\"Main function to run the complete QAT pipeline\"\"\"\n    \n    print(\"Starting QAT ViT Large Training Pipeline on CIFAR-100...\")\n    print(\"=\"*70)\n    \n    # Configuration\n    batch_size = 16   # Smaller batch size due to ViT Large memory requirements\n    num_epochs = 2    # Reduced for demonstration\n    learning_rate = 5e-6  # Lower learning rate for large model\n    num_classes = 100\n    \n    # Prepare data\n    print(\"Preparing CIFAR-100 dataset...\")\n    train_loader, test_loader = prepare_data(batch_size=batch_size)\n    \n    # CIFAR-100 class names (100 fine-grained classes)\n    class_names = [\n        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n        'worm'\n    ]\n    \n    # Initialize metrics tracker\n    metrics_tracker = PerformanceMetrics()\n    \n    # Check device availability\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    print(f\"ViT Model: Large (vit_large_patch16_224)\")\n    print(f\"Dataset: CIFAR-100 ({num_classes} classes)\")\n    \n    # ========================\n    # PART 1: FP32 Model Training and Evaluation\n    # ========================\n    print(\"\\n\" + \"=\"*70)\n    print(\"PART 1: FP32 ViT LARGE MODEL TRAINING\")\n    print(\"=\"*70)\n    \n    # Create FP32 model\n    model_fp32 = create_vit_model(num_classes)\n    print(f\"Model parameters: {sum(p.numel() for p in model_fp32.parameters())/1e6:.1f}M\")\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer_fp32 = optim.AdamW(model_fp32.parameters(), lr=learning_rate, weight_decay=0.01)\n    \n    # Train FP32 model\n    train_model(model_fp32, train_loader, criterion, optimizer_fp32, device, num_epochs)\n    \n    # Evaluate FP32 model\n    print(\"\\nEvaluating FP32 model...\")\n    model_fp32.eval()\n    fp32_accuracy, fp32_inf_time, fp32_preds, fp32_targets = evaluate_model(\n        model_fp32, test_loader, device, num_classes\n    )\n    \n    # Get FP32 model metrics\n    fp32_size = get_model_size(model_fp32)\n    fp32_memory = get_memory_usage()\n    \n    metrics_tracker.add_model_metrics(\n        'FP32', fp32_accuracy, fp32_inf_time, fp32_size, fp32_memory\n    )\n    \n    print(f\"FP32 ViT Large - Accuracy: {fp32_accuracy:.4f}, \"\n          f\"Avg Inference Time: {fp32_inf_time*1000:.2f}ms, \"\n          f\"Model Size: {fp32_size:.2f}MB\")\n    \n    # ========================\n    # PART 2: QAT Model Training\n    # ========================\n    print(\"\\n\" + \"=\"*70)\n    print(\"PART 2: QAT ViT LARGE MODEL TRAINING\")\n    print(\"=\"*70)\n    \n    # Move model to CPU for quantization preparation\n    model_fp32.to(\"cpu\")\n    \n    # Prepare example inputs for FX graph mode quantization\n    example_inputs = (torch.randn(1, 3, 224, 224),)\n    \n    # Get QAT configuration\n    qconfig_mapping = torch.ao.quantization.get_default_qat_qconfig_mapping(\"x86\")\n    \n    # Prepare model for QAT\n    print(\"Preparing ViT Large for QAT...\")\n    model_prepared = quantize_fx.prepare_qat_fx(model_fp32, qconfig_mapping, example_inputs)\n    \n    # Move back to training device\n    model_prepared.to(device)\n    \n    # Create new optimizer for QAT training (even lower learning rate)\n    optimizer_qat = optim.AdamW(model_prepared.parameters(), lr=learning_rate/10, weight_decay=0.01)\n    \n    # QAT Training (typically fewer epochs)\n    print(\"Starting QAT fine-tuning for ViT Large...\")\n    train_model(model_prepared, train_loader, criterion, optimizer_qat, device, num_epochs)\n    \n    # ========================\n    # PART 3: Convert to INT8 and Evaluate\n    # ========================\n    print(\"\\n\" + \"=\"*70)\n    print(\"PART 3: INT8 CONVERSION AND EVALUATION\")\n    print(\"=\"*70)\n    \n    # Convert to INT8 (must be done on CPU)\n    model_prepared.to(\"cpu\")\n    model_prepared.eval()\n    \n    print(\"Converting ViT Large to INT8 model...\")\n    model_int8_qat = quantize_fx.convert_fx(model_prepared)\n    \n    # Evaluate INT8 model\n    print(\"Evaluating INT8 QAT ViT Large model...\")\n    qat_accuracy, qat_inf_time, qat_preds, qat_targets = evaluate_model(\n        model_int8_qat, test_loader, \"cpu\", num_classes\n    )\n    \n    # Get QAT model metrics\n    qat_size = get_model_size(model_int8_qat)\n    qat_memory = get_memory_usage()\n    \n    metrics_tracker.add_model_metrics(\n        'QAT_INT8', qat_accuracy, qat_inf_time, qat_size, qat_memory\n    )\n    \n    print(f\"QAT INT8 ViT Large - Accuracy: {qat_accuracy:.4f}, \"\n          f\"Avg Inference Time: {qat_inf_time*1000:.2f}ms, \"\n          f\"Model Size: {qat_size:.2f}MB\")\n    \n    # ========================\n    # PART 4: Performance Comparison and Analysis\n    # ========================\n    print(\"\\n\" + \"=\"*70)\n    print(\"PART 4: PERFORMANCE ANALYSIS - ViT LARGE on CIFAR-100\")\n    print(\"=\"*70)\n    \n    # Display comprehensive comparison\n    metrics_tracker.compare_models()\n    \n    # Generate top-5 accuracy for CIFAR-100\n    print(\"\\n\" + \"=\"*50)\n    print(\"TOP-5 ACCURACY ANALYSIS:\")\n    print(\"=\"*50)\n    \n    def calculate_top5_accuracy(model, data_loader, device):\n        model.eval()\n        correct_top5 = 0\n        total = 0\n        \n        with torch.no_grad():\n            for data, target in data_loader:\n                data, target = data.to(device), target.to(device)\n                outputs = model(data)\n                _, pred = outputs.topk(5, 1, True, True)\n                pred = pred.t()\n                correct = pred.eq(target.view(1, -1).expand_as(pred))\n                correct_top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True).item()\n                total += target.size(0)\n        \n        return correct_top5 / total\n    \n    # Calculate top-5 accuracy for both models\n    model_fp32.to(device)\n    fp32_top5 = calculate_top5_accuracy(model_fp32, test_loader, device)\n    qat_top5 = calculate_top5_accuracy(model_int8_qat, test_loader, \"cpu\")\n    \n    print(f\"FP32 ViT Large Top-5 Accuracy: {fp32_top5:.4f}\")\n    print(f\"QAT INT8 ViT Large Top-5 Accuracy: {qat_top5:.4f}\")\n    print(f\"Top-5 Accuracy Retention: {(qat_top5/fp32_top5)*100:.2f}%\")\n    \n    # Generate classification reports (showing top 10 classes for brevity)\n    print(\"\\n\" + \"=\"*50)\n    print(\"SAMPLE CLASSIFICATION METRICS (First 10 classes):\")\n    print(\"=\"*50)\n    \n    from sklearn.metrics import classification_report\n    sample_classes = class_names[:10]\n    \n    # Filter predictions and targets for first 10 classes only\n    fp32_sample_mask = [i for i, target in enumerate(fp32_targets) if target < 10]\n    qat_sample_mask = [i for i, target in enumerate(qat_targets) if target < 10]\n    \n    if fp32_sample_mask and qat_sample_mask:\n        fp32_sample_preds = [fp32_preds[i] for i in fp32_sample_mask]\n        fp32_sample_targets = [fp32_targets[i] for i in fp32_sample_mask]\n        qat_sample_preds = [qat_preds[i] for i in qat_sample_mask]\n        qat_sample_targets = [qat_targets[i] for i in qat_sample_mask]\n        \n        print(\"\\nFP32 ViT Large (Sample Classes):\")\n        print(classification_report(fp32_sample_targets, fp32_sample_preds, \n                                  target_names=sample_classes, labels=list(range(10))))\n        \n        print(\"\\nQAT INT8 ViT Large (Sample Classes):\")\n        print(classification_report(qat_sample_targets, qat_sample_preds, \n                                  target_names=sample_classes, labels=list(range(10))))\n    \n    # Save models\n    print(\"\\nSaving models...\")\n    torch.save(model_fp32.state_dict(), 'vit_large_fp32_cifar100.pth')\n    torch.save(model_int8_qat.state_dict(), 'vit_large_qat_int8_cifar100.pth')\n    \n    print(\"\\nQAT ViT Large Training Pipeline on CIFAR-100 Completed!\")\n    print(\"Models saved successfully.\")\n    print(f\"Final Model Size Reduction: {((fp32_size - qat_size) / fp32_size) * 100:.1f}%\")\n    \n    return model_fp32, model_int8_qat, metrics_tracker\n\nif __name__ == \"__main__\":\n    # Run the complete pipeline\n    fp32_model, qat_model, metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:09:15.133152Z","iopub.execute_input":"2025-08-28T09:09:15.133325Z","iopub.status.idle":"2025-08-28T10:57:32.366157Z","shell.execute_reply.started":"2025-08-28T09:09:15.133310Z","shell.execute_reply":"2025-08-28T10:57:32.364781Z"}},"outputs":[{"name":"stdout","text":"Starting QAT ViT Large Training Pipeline on CIFAR-100...\n======================================================================\nPreparing CIFAR-100 dataset...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169M/169M [00:10<00:00, 15.9MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nViT Model: Large (vit_large_patch16_224)\nDataset: CIFAR-100 (100 classes)\n\n======================================================================\nPART 1: FP32 ViT LARGE MODEL TRAINING\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f4ecac4745443a88e1cb3d544e17d5"}},"metadata":{}},{"name":"stdout","text":"Model parameters: 303.4M\n\nTraining on cuda for 2 epochs...\nEpoch [1/2], Step [100], Loss: 4.7780, Acc: 6.31%\nEpoch [1/2], Step [200], Loss: 3.1825, Acc: 18.38%\nEpoch [1/2], Step [300], Loss: 1.7653, Acc: 33.04%\nEpoch [1/2], Step [400], Loss: 1.0600, Acc: 44.03%\nEpoch [1/2], Step [500], Loss: 0.8330, Acc: 51.30%\nEpoch [1/2], Step [600], Loss: 0.7020, Acc: 56.52%\nEpoch [1/2], Step [700], Loss: 0.5989, Acc: 60.57%\nEpoch [1/2], Step [800], Loss: 0.5546, Acc: 63.77%\nEpoch [1/2], Step [900], Loss: 0.5199, Acc: 66.35%\nEpoch [1/2], Step [1000], Loss: 0.4918, Acc: 68.45%\nEpoch [1/2], Step [1100], Loss: 0.5095, Acc: 70.08%\nEpoch [1/2], Step [1200], Loss: 0.4116, Acc: 71.71%\nEpoch [1/2], Step [1300], Loss: 0.4674, Acc: 72.92%\nEpoch [1/2], Step [1400], Loss: 0.4224, Acc: 74.02%\nEpoch [1/2], Step [1500], Loss: 0.4000, Acc: 75.05%\nEpoch [1/2], Step [1600], Loss: 0.3587, Acc: 75.97%\nEpoch [1/2], Step [1700], Loss: 0.4117, Acc: 76.75%\nEpoch [1/2], Step [1800], Loss: 0.4085, Acc: 77.42%\nEpoch [1/2], Step [1900], Loss: 0.3544, Acc: 78.08%\nEpoch [1/2], Step [2000], Loss: 0.3915, Acc: 78.65%\nEpoch [1/2], Step [2100], Loss: 0.3719, Acc: 79.20%\nEpoch [1/2], Step [2200], Loss: 0.4016, Acc: 79.66%\nEpoch [1/2], Step [2300], Loss: 0.3815, Acc: 80.09%\nEpoch [1/2], Step [2400], Loss: 0.3441, Acc: 80.51%\nEpoch [1/2], Step [2500], Loss: 0.3745, Acc: 80.87%\nEpoch [1/2], Step [2600], Loss: 0.3370, Acc: 81.25%\nEpoch [1/2], Step [2700], Loss: 0.3263, Acc: 81.59%\nEpoch [1/2], Step [2800], Loss: 0.3506, Acc: 81.88%\nEpoch [1/2], Step [2900], Loss: 0.3484, Acc: 82.18%\nEpoch [1/2], Step [3000], Loss: 0.3146, Acc: 82.49%\nEpoch [1/2], Step [3100], Loss: 0.3281, Acc: 82.77%\nEpoch [1/2] completed. Training Accuracy: 82.82%\nEpoch [2/2], Step [100], Loss: 0.2135, Acc: 93.44%\nEpoch [2/2], Step [200], Loss: 0.2070, Acc: 94.16%\nEpoch [2/2], Step [300], Loss: 0.2574, Acc: 93.65%\nEpoch [2/2], Step [400], Loss: 0.2142, Acc: 93.64%\nEpoch [2/2], Step [500], Loss: 0.2152, Acc: 93.67%\nEpoch [2/2], Step [600], Loss: 0.2314, Acc: 93.60%\nEpoch [2/2], Step [700], Loss: 0.2087, Acc: 93.64%\nEpoch [2/2], Step [800], Loss: 0.2218, Acc: 93.65%\nEpoch [2/2], Step [900], Loss: 0.2390, Acc: 93.62%\nEpoch [2/2], Step [1000], Loss: 0.2507, Acc: 93.53%\nEpoch [2/2], Step [1100], Loss: 0.2408, Acc: 93.50%\nEpoch [2/2], Step [1200], Loss: 0.2366, Acc: 93.51%\nEpoch [2/2], Step [1300], Loss: 0.2049, Acc: 93.51%\nEpoch [2/2], Step [1400], Loss: 0.2164, Acc: 93.51%\nEpoch [2/2], Step [1500], Loss: 0.2346, Acc: 93.52%\nEpoch [2/2], Step [1600], Loss: 0.2095, Acc: 93.54%\nEpoch [2/2], Step [1700], Loss: 0.2227, Acc: 93.51%\nEpoch [2/2], Step [1800], Loss: 0.2645, Acc: 93.46%\nEpoch [2/2], Step [1900], Loss: 0.2392, Acc: 93.45%\nEpoch [2/2], Step [2000], Loss: 0.2414, Acc: 93.46%\nEpoch [2/2], Step [2100], Loss: 0.2088, Acc: 93.49%\nEpoch [2/2], Step [2200], Loss: 0.2713, Acc: 93.41%\nEpoch [2/2], Step [2300], Loss: 0.2259, Acc: 93.39%\nEpoch [2/2], Step [2400], Loss: 0.2241, Acc: 93.38%\nEpoch [2/2], Step [2500], Loss: 0.2286, Acc: 93.36%\nEpoch [2/2], Step [2600], Loss: 0.2241, Acc: 93.37%\nEpoch [2/2], Step [2700], Loss: 0.2513, Acc: 93.36%\nEpoch [2/2], Step [2800], Loss: 0.2100, Acc: 93.37%\nEpoch [2/2], Step [2900], Loss: 0.2311, Acc: 93.37%\nEpoch [2/2], Step [3000], Loss: 0.2065, Acc: 93.39%\nEpoch [2/2], Step [3100], Loss: 0.1889, Acc: 93.42%\nEpoch [2/2] completed. Training Accuracy: 93.41%\n\nEvaluating FP32 model...\nFP32 ViT Large - Accuracy: 0.9356, Avg Inference Time: 12.76ms, Model Size: 1157.39MB\n\n======================================================================\nPART 2: QAT ViT LARGE MODEL TRAINING\n======================================================================\nPreparing ViT Large for QAT...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Starting QAT fine-tuning for ViT Large...\n\nTraining on cuda for 2 epochs...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3366323004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# Run the complete pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mfp32_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/3366323004.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# QAT Training (typically fewer epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting QAT fine-tuning for ViT Large...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_qat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# ========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3164937032.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_wrapped\u001b[0m  \u001b[0;31m# type: ignore[method-assign]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: B904\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<eval_with_key>.2\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mactivation_post_process_571\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process_571\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks_21_ls1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mblocks_21_ls1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0mblocks_21_drop_path1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"21\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_post_process_571\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mactivation_post_process_571\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m     \u001b[0mactivation_post_process_572\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process_572\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks_21_drop_path1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mblocks_21_drop_path1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0madd_43\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_post_process_557\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactivation_post_process_572\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mactivation_post_process_557\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_post_process_572\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0mactivation_post_process_573\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process_573\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_43\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0madd_43\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fake_quantize.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         return torch.fused_moving_avg_obs_fake_quant(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserver_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 25.12 MiB is free. Process 2508 has 15.86 GiB memory in use. Of the allocated memory 14.84 GiB is allocated by PyTorch, and 741.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 25.12 MiB is free. Process 2508 has 15.86 GiB memory in use. Of the allocated memory 14.84 GiB is allocated by PyTorch, and 741.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":4}]}
